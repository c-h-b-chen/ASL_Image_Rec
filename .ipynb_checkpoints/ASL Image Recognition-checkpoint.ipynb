{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Before building our model, we need to transform our dataset of 81,000 200x200 images in these ways:\n",
    "1. Convert to grayscale\n",
    "2. Normalize the exposure/brightness\n",
    "3. Resize to 75x75\n",
    "4. Extract the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','nothing']\n",
    "\n",
    "transformed_data = 'transformed_data.csv'\n",
    "img_dim = 75\n",
    "\n",
    "with open(transformed_data, newline='', mode='w') as file:\n",
    "    file_writer = csv.writer(file, delimiter=',')\n",
    "    for i in range(len(letters)):\n",
    "        for j in range(0, 3000):\n",
    "            # Pillow image module used to open file and convert to grayscale\n",
    "            img = Image.open('../asl_alphabet_train/'+str(letters[i])+'+/'+str(letters[i])+str(j+1)+'+.jpg').convert('L')\n",
    "            brightness = (ImageStat.Stat(img)).rms[0]\n",
    "\n",
    "            if brightness > 130:\n",
    "                img_bright = img.point(lambda p: p * 0.8)\n",
    "\n",
    "            elif brightness <= 130:\n",
    "                    img_bright = img.point(lambda p: p * 2.0)\n",
    "\n",
    "            img_bright.show()\n",
    "            img = np.asarray(img_bright)\n",
    "            upper_thresh = 25\n",
    "            lower_thresh = 20\n",
    "\n",
    "            edges = cv2.Canny(img, lower_thresh, upper_thresh)\n",
    "            res = cv2.resize(edges, dsizde=(img_dim,img_dim), interpolation=cv2.INTER_CUBIC)\n",
    "            imgArray = np.array(res)\n",
    "            flattened = np.ravel(imgArray)\n",
    "            incLabel = np.append(i, flattened)\n",
    "\n",
    "            file_writer.writerow(incLabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: transformed_data.csv will come out to a little over 1G in size.\n",
    "\n",
    "Shuffle the dataset to promote model integrity down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../transformed_data.csv')\n",
    "dataset = dataset.iloc[np.random.permutation(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n",
      "X_train: (60749, 5625), X_test: (20250, 5625), Y_train: (60749,), Y_test(20250,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[:, 1:], dataset.iloc[:,0],\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "print(\"X_train: {0}, X_test: {1}, Y_train: {2}, Y_test{3}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components discovered after running pca = PCA(.85) on a subset of the dataset. Grid search and cross validation was used to help us determine the best parameters to test with.\n",
    "\n",
    "Processing a dataset as large as this can be very memory intensive. Incremental PCA allows us to find our components without crashing when our resources are limited. To test the fidelity of the models, we can pickle our trained files to test on different machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming with PCA...\n",
      "X PCA Train: (60749, 3000) X PCA Test: (20250, 3000)\n",
      "Time elapsed: 7.11 min\n"
     ]
    }
   ],
   "source": [
    "print('Transforming with PCA...')\n",
    "pca = IncrementalPCA(n_components=3000)\n",
    "\n",
    "start = time.time()\n",
    "pca.fit(X_train)\n",
    "X_trainsformed = pca.transform(X_train)\n",
    "X_testformed = pca.transform(X_test)\n",
    "end = time.time()\n",
    "print('X PCA Train: {0} X PCA Test: {1}'.format(X_trainsformed.shape, X_testformed.shape))\n",
    "print('Time elapsed: {} min'.format(round((end-start)/60, 2)))\n",
    "\n",
    "pca_pkl = open('pca_ASL.pkl', 'wb')\n",
    "pickle.dump(pca, pca_pkl)\n",
    "pca_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM...\n",
      "Time elapsed (SVM): 59.32 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fitting SVM...')\n",
    "clf = SVC(C=1e-07, kernel='linear')\n",
    "start2 = time.time()\n",
    "clf.fit(X_trainsformed, y_train)\n",
    "end2 = time.time()\n",
    "print('Time elapsed (SVM): {} min\\n'.format(round((end2-start2)/60, 2)))\n",
    "\n",
    "svm_pkl = open('svm_ASL.pkl', 'wb')\n",
    "pickle.dump(clf, svm_pkl)\n",
    "svm_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8699753086419753\n",
      "Accuracy: 0.8699753086419753\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_testformed)\n",
    "print('Score: ', clf.score(X_testformed, y_test))\n",
    "print(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
